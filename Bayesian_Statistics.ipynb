{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#traditional stuff\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that we generally need for cross-validation and plotting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "def cv_optimise(clf, Xtrain, Ytrain, params, n_folds = 5):\n",
    "        gs = GridSearchCV(clf, param_grid = params, cv = n_folds)\n",
    "        gs.fit(Xtrain, Ytrain)\n",
    "        print (\"BEST PARAMS\", gs.best_params_)\n",
    "        best = gs.best_estimator_\n",
    "        return best\n",
    "def do_classify(clf, params, df, features, target_name, target_value, mask = None, standardise = False, \n",
    "                train_size = 0.8):\n",
    "    #targetname is one of the features only\n",
    "    subdf = df[features]\n",
    "    #standarisation and normalisation are two different ways to bring the data to scale\n",
    "    if standardise:\n",
    "        subdf = (subdf - subdf.mean()) / subdf.std()\n",
    "    else:\n",
    "        subdf = subdf\n",
    "    X = subdf.values\n",
    "    y = (subdf[target_name].values == target_value) * 1\n",
    "    Xtrain, xtest, Ytrain, ytest = train_test_split(X, y, train_size = train_size)\n",
    "    clf = cv_optimise(clf, Xtrain, Ytrain, params)\n",
    "    #after finding the best classifier with best params, we again fit the train and the test data \n",
    "    clf = clf.fit(Xtrain, Ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, Ytrain)\n",
    "    testing_accuracy = clf.score(Xtest, ytest)\n",
    "    print (\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "    print (\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def points_plot(ax, Xtrain, Xtest, Ytrain, Ytest, clf, mesh = True, colorscale=cmap_light,\n",
    "                cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False, predicted=False):\n",
    "    h = .02\n",
    "    #x has heights and weights \n",
    "    X = np.concatenate(Xtrain, Xtest)\n",
    "    #heights max and min\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    #to create points for the meshgrid\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    if zfunc:\n",
    "        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        z = zfunc(p0, p1)\n",
    "    else:\n",
    "        z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    ZZ = z.reshape(xx.shape)\n",
    "    if mesh:\n",
    "        plt.pcolormesh(xx, yy, ZZ, cmap = cmap_light, alpha = alpha, axes = ax)\n",
    "    if predicted:\n",
    "        #if predicted is true show the predicted values on the map\n",
    "        showtr = clf.predict(Xtrain)\n",
    "        showte = clf.predict(Xtest)\n",
    "    else:\n",
    "        #else just show the true values \n",
    "        showtr = Ytrain\n",
    "        showte = Ytest    \n",
    "    #s is the area of one data point irrespective of the shape   \n",
    "    #and c is the color list\n",
    "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n",
    "    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    return ax,xx,yy\n",
    "\n",
    "def points_plot_proba(ax, Xtrain, Xtest, Ytrain, Ytest, clf, colorscale = cmap_light,\n",
    "                     cdiscrete = cmap_bold, ccolor = cm, psize = 10, alpha = 0.1):\n",
    "    #get the mesh points\n",
    "    ax, xx, yy = points_plot(ax, Xtrain, Xtest, Ytrain, Ytest, clf, mesh = False, colorscale = colorscale,\n",
    "                            cdiscrete = cdiscrete, alpha = alpha, psize = psize)\n",
    "    z = clf.predict_proba(np.c_(xx.ravel(), yy.ravel()))\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, z, cmap = ccolor, alpha = .2, axes = ax)\n",
    "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14, axes=ax)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorm is simply read as - \n",
    " \n",
    "** The probability of posterior is directly proposional to the product of liklihood and the prior** posterior being the probability when we have the data and prior being the probabilty which we estimate before hand(using different distributions like Beta, Gamma distributions)\n",
    "\n",
    "For more info - \"Think Bayes\" is a very nice book!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "(27450, 27450, 29420,\"10/10/2016\"),\n",
    "(29420 , 36142, 29420, \"10/10/2016\"),\n",
    "(11 , 11, 27450, \"10/10/2016\")] \n",
    "\n",
    "#Create DataFrame base\n",
    "df = pd.DataFrame(data, columns=(\"User_id\",\"Actor1\",\"Actor2\", \"Time\"))\n",
    "df[\"Col1\"] = [1 if i in df[\"Actor1\"].values else 0 for i in df[\"User_id\"].values]\n",
    "df[\"Col2\"] = [df.iloc[i][\"Actor2\"] if j == 1 else df.iloc[i][\"Actor1\"] for i, j in enumerate(df[\"Col1\"].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User_id  Actor1  Actor2        Time  Col1   Col2\n",
      "0    27450   27450   29420  10/10/2016     1  29420\n",
      "1    29420   36142   29420  10/10/2016     0  36142\n",
      "2       11      11   27450  10/10/2016     1  27450\n"
     ]
    }
   ],
   "source": [
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
