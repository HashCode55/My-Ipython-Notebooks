{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- means a new topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is another type of classifier which seprates the data-point in the feature space using hyperplanes. When we look at the graph showing SVM there is a weight vector which lets us change the orientation of the hyperplane and the bias which lets us change the distance of hyperplane from the origin. **So our task is to find the appropriate bias and the weight vector so that the datapoints get seprated in two classes!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in 1957 Frank Rosenblatt modeled the equation wx + b = 0 as a perceptron and his hypothesis was sadly rejected(*because of the XOR problem*). But then in recent years a concept of adding minimum dimention to seprate the datapoints by doing a trick and using a hyperplane revived it which is called deep learning now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something is weird about SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the motive of support vectors machines is to find maximum margin hyperplane, and the data points which touch this hyperplane(*which'll be the border points*) are called the support vectors. **So SVM only depend on the border cases**\n",
    "Which is weird! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Really, really refer to the slides!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----Masking the dataset to use in train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example - \n",
    "    `itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.6)\n",
    "    mask=np.ones(df.shape[0], dtype='int')\n",
    "    mask[itrain]=1\n",
    "    mask[itest]=0\n",
    "    mask = (mask==1)\n",
    "    mask[:10]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----Logistic Regression imp. notes \n",
    "\n",
    "While doing Logistic regression in sklearn.linearmodel, clf.predict returns a numpy array of 0s and 1s by taking the threshhold probabily of 0.5, which is not a good idea if the dataset is imbalanced, whereas predict_proba gives exact probability of 1 and 0s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Some reusable code for decision tree plotting and cross validation(explantion included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# special matplotlib argument for improved plots\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# Generic classification and optimization functions from last lab\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# clf - original classifier\n",
    "# parameters - grid to search over\n",
    "# X - usually your training X matrix\n",
    "# y - usually your training y \n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
    "    #if the score function is supplied use that in the grid search\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
    "    #fit the model     \n",
    "    gs.fit(X, y)\n",
    "    print (\"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_)\n",
    "    best = gs.best_estimator_\n",
    "    return best\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# Important parameters\n",
    "# indf - Input dataframe\n",
    "# featurenames - vector of names of predictors\n",
    "# targetname - name of column you want to predict (e.g. 0 or 1, 'M' or 'F', \n",
    "#              'yes' or 'no')\n",
    "# target1val - particular value you want to have as a 1 in the target\n",
    "# mask - boolean vector indicating test set (~mask is training set)\n",
    "# reuse_split - dictionary that contains traning and testing dataframes \n",
    "#              (we'll use this to test different classifiers on the same \n",
    "#              test-train splits)\n",
    "# score_func - we've used the accuracy as a way of scoring algorithms but \n",
    "#              this can be more general later on\n",
    "# n_folds - Number of folds for cross validation ()\n",
    "# n_jobs - used for parallelization\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask=None, reuse_split=None, score_func=None, n_folds=5, n_jobs=1):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    if mask !=None:\n",
    "        print (\"using mask\")\n",
    "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if reuse_split !=None:\n",
    "        print (\"using reuse split\")\n",
    "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_jobs=n_jobs, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print (\"############# based on standard predict ################\")\n",
    "    print (\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "    print (\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n",
    "    print (confusion_matrix(ytest, clf.predict(Xtest)))\n",
    "    print (\"########################################################\")\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "# Plot tree containing only two covariates\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "# cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def plot_2tree(ax, Xtr, Xte, ytr, yte, clf, plot_train = True, plot_test = True, lab = ['Feature 1', 'Feature 2'], mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.3, psize=10, zfunc=False):\n",
    "    # Create a meshgrid as our test data\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plot_step= 0.05\n",
    "    xmin, xmax= Xtr[:,0].min(), Xtr[:,0].max()\n",
    "    ymin, ymax= Xtr[:,1].min(), Xtr[:,1].max()\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, plot_step), np.arange(ymin, ymax, plot_step) )\n",
    "\n",
    "    # Re-cast every coordinate in the meshgrid as a 2D point\n",
    "    Xplot= np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "    # Predict the class\n",
    "    Z = clfTree1.predict( Xplot )\n",
    "\n",
    "    # Re-shape the results\n",
    "    Z= Z.reshape( xx.shape )\n",
    "    cs = plt.contourf(xx, yy, Z, cmap= cmap_light, alpha=0.3)\n",
    "  \n",
    "    # Overlay training samples\n",
    "    if (plot_train == True):\n",
    "        plt.scatter(Xtr[:, 0], Xtr[:, 1], c=ytr-1, cmap=cmap_bold, alpha=alpha,edgecolor=\"k\") \n",
    "    # and testing points\n",
    "    if (plot_test == True):\n",
    "        plt.scatter(Xte[:, 0], Xte[:, 1], c=yte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\")\n",
    "\n",
    "    plt.xlabel(lab[0])\n",
    "    plt.ylabel(lab[1])\n",
    "    plt.title(\"Boundary for decision tree classifier\",fontsize=7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2],\n",
       "        [2, 2],\n",
       "        [1, 3],\n",
       "        [2, 3]]), array([[1, 2],\n",
       "        [1, 2]]), array([[2, 2],\n",
       "        [3, 3]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a little bit explanation about ravel, c_, meshgrid\n",
    "#meshgrid is a function used for doing vectorized operations as seen in the output\n",
    "xx, yy = np.meshgrid([1, 2], [2, 3])\n",
    "#_c differs from concatenes in a simple way, it concatenates horizontally while the other does is vertically\n",
    "#ravel is just used to flatten up a np ndarray\n",
    "#now what is happening in the above code is, xx.ravel flattens up the xx grid, which is just repetitions of the \n",
    "#first list... and yy ravel flattens up the yy grid which looks like below\n",
    "#now when they are c_'d, xx can be thought of lots of x axis layers and yy can be thought of y points corresponding to \n",
    "#each xx point, which'll fill the plot with a mesh with min and max bounds!\n",
    "np.c_[xx.ravel(), yy.ravel()], xx, yy\n",
    "#contourf does the above things on its own when gives xx and yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
